# Meal Planner - AI Coding Agent Instructions

## Important: Collaboration Guidelines

You are collaborating with a human who may make changes between your edits:

- **Always re-verify** file contents before making changes - don't assume previous state
- **If your previous changes are gone**, do not re-add them without checking with the user first
- **Read before editing** - the human may have modified, moved, or intentionally removed content
- **Verify suggestions** - when given review comments or suggestions, verify they are correct against actual code before applying
- **Compare alternatives** - when the user suggests a different approach, analyze both options and explain the tradeoffs
- **Troubleshoot step-by-step** - when debugging, suggest one fix at a time and wait for results
- **Plan before large changes** - for complex changes (3+ files), propose a high-level plan first
- **Never work directly on main** - Always create a feature branch for changes

## Keeping Instructions Current

When reviewing PRs, check if changes affect this documentation:

- New/renamed/removed workflows in `.github/workflows/`
- Changes to code style tools (ruff, pre-commit hooks, etc.)
- New dependencies or architectural patterns
- Changes to testing patterns or conventions

If any of these change, suggest updating `.github/copilot-instructions.md` in your review.

## Project Overview

**Meal Planner** is a recipe collector and weekly meal planner app built with Streamlit. It allows users to:

1. **Import recipes** from URLs - automatically extracts ingredients, instructions, and images
2. **Plan weekly meals** - organize recipes into a weekly calendar
3. **Generate grocery lists** - combine ingredients from planned meals

## Architecture

### Application Structure

```
app/
├── main.py              # Streamlit app entry point
├── models/              # Data models (dataclasses)
│   ├── recipe.py        # Recipe model
│   ├── meal_plan.py     # MealPlan, PlannedMeal models
│   └── grocery_list.py  # GroceryList, GroceryItem models
├── services/            # Business logic
│   └── recipe_scraper.py    # Extract recipes from URLs using recipe-scrapers
└── storage/             # Data persistence (future)
```

### Key Dependencies

- **streamlit** - Web UI framework
- **recipe-scrapers** - Library for extracting recipes from 400+ websites
- **httpx** - HTTP client for fetching recipe pages
- **pillow** - Image processing (for recipe photos)

### Data Flow

1. User pastes recipe URL → `recipe_scraper.scrape_recipe()` fetches and parses
2. Recipe stored in session state (future: persistent storage)
3. User plans meals → MealPlan created with PlannedMeal entries
4. Grocery list generated by combining ingredients from planned recipes

## Development Workflows

### Running the App

```bash
uv sync                              # Install dependencies
uv run streamlit run app/main.py     # Run the app
```

### Code Quality Tools

- **Package manager**: UV (Astral's fast Python package manager)
- **Linter/Formatter**: Ruff (configured in `pyproject.toml`)
  - Line length: 120 characters
  - Target: Python 3.14
- **Pre-commit hooks**: `.pre-commit-config.yaml`
  - Install: `uv run pre-commit install`
- **Testing**: pytest with coverage
  - Run: `uv run pytest --cov=app`
- **Conventional commits**: `feat:`, `fix:`, `chore:`, `ci:`, `docs:`, `refactor:`, `test:`

## Code Style

See `pyproject.toml` for tool configurations.

- **Ruff** for linting/formatting
- Test files must match `test_*.py` pattern
- **Self-documenting code**: Avoid inline comments - code should be readable without them
- **Test coverage**: All new methods and modified functions must have corresponding tests
- **Dataclasses**: Use dataclasses for models with `@dataclass` decorator
- **Type hints**: Use modern Python type hints (`list[str]` not `List[str]`)

## Test-Driven Development (TDD)

**Strongly prefer TDD when implementing new features or modules.**

### When to Use TDD

- **New models** - Write tests first to define the data structure
- **New services** - Test edge cases before implementation
- **Bug fixes** - Write a failing test that reproduces the bug
- **Refactoring** - Ensure tests pass before and after

### Running Tests

```bash
uv run pytest -n=auto --cov=app --cov-report=term-missing
uv run pytest tests/test_recipe.py -v
```

## Key Patterns & Conventions

### Recipe Scraping

- Uses `recipe-scrapers` library which supports 400+ recipe websites
- Always handle scraping errors gracefully - return `None` if scraping fails
- Extract: title, ingredients, instructions, image_url, servings, prep_time, cook_time

### Streamlit Patterns

- Use `st.session_state` for persisting data across reruns
- Use `st.spinner()` for long-running operations like HTTP requests
- Use `st.form()` for grouping inputs that should submit together
- Handle errors with `st.error()` and successes with `st.success()`

### Data Models

- Use Python dataclasses for all models
- Default mutable fields with `field(default_factory=list)`
- Add computed properties for derived values (e.g., `total_time_calculated`)

## When Making Changes

### Adding New Features

1. Create feature branch: `git checkout -b feat/feature-name`
2. Write tests first (TDD)
3. Implement the feature
4. Run tests and linting
5. Commit with conventional commit message
6. Push and create PR

### Adding Recipe Scraper Features

- Test with real recipe URLs from different sites
- Handle missing fields gracefully (not all sites have all info)
- Consider caching fetched recipes

### Adding Grocery List Features

- Implement ingredient parsing/normalization
- Group by category (produce, dairy, etc.)
- Handle quantity merging for duplicate items

## Future Enhancements

- [ ] Persistent storage (JSON files or SQLite)
- [ ] Ingredient parsing and normalization
- [ ] Category detection for grocery items
- [ ] Recipe editing and manual entry
- [ ] Export grocery list to various formats
- [ ] User accounts and cloud sync
